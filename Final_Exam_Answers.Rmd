---
title: "DATA 606 Spring 2018 - Final Exam"
author: "Brian Weinfeld"
output: pdf_document
---

```{r include=FALSE}
library(tidyverse)
library(magrittr)
```
# Part I

Please put the answers for Part I next to the question number (2pts each):

1. B
2. B
3. A
4. C
5. B
6. D

7a. Describe the two distributions (2pts).

Center: Both distributions have a similar mean.
Shape: Distribution A is skew-right while distribution B is roughly normal. 
Spread: A has a much larger spread, with a range of $\approx18$, while B has a much smaller spread, with a range
of $\approx3.5$

7b. Explain why the means of these two distributions are similar but the standard deviations are not (2 pts).

The means are the same because the sampling distribution is used to approximate the mean of distribution A. As more samples are added to the sampling distribution
it will begin to converge on the true mean of 5.05. This also explains why the sampling distribution has a smaller standard deviation. That is, the distribution is
being used to approximate the mean of distribution A, not any of the other points. As more samples are added to B, the standard deviation will continue to shrink.

7c. What is the statistical principal that describes this phenomenon (2 pts)?

The statistical principal in effect with the sampling distribution is the __Central Limit Theorem__.

# Part II

Consider the four datasets, each with two columns (x and y), provided below. 

```{r}
data1 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68))
data2 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74))
data3 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73))
data4 <- data.frame(x=c(8,8,8,8,8,8,8,19,8,8,8),
					y=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89))
```

For each column, calculate (to two decimal places):

#### a. The mean (for x and y separately; 1 pt).

Data 1
```{r}
mean(data1$x) %>% round(2)
mean(data1$y) %>% round(2)
```

Data 2

```{r}
mean(data2$x) %>% round(2)
mean(data2$y) %>% round(2)
```

Data 3

```{r}
mean(data3$x) %>% round(2)
mean(data3$y) %>% round(2)
```

Data 4

```{r}
mean(data4$x) %>% round(2)
mean(data4$y) %>% round(2)
```

#### b. The median (for x and y separately; 1 pt).

Data 1

```{r}
median(data1$x) %>% round(2)
median(data1$y) %>% round(2)
```

Data 2

```{r}
median(data2$x) %>% round(2)
median(data2$y) %>% round(2)
```

Data 3

```{r}
median(data3$x) %>% round(2)
median(data3$y) %>% round(2)
```

Data 4

```{r}
median(data4$x) %>% round(2)
median(data4$y) %>% round(2)
```

#### c. The standard deviation (for x and y separately; 1 pt).

Data 1

```{r}
sd(data1$x) %>% round(2)
sd(data1$y) %>% round(2)
```

Data 2

```{r}
sd(data2$x) %>% round(2)
sd(data2$y) %>% round(2)
```

Data 3

```{r}
sd(data3$x) %>% round(2)
sd(data3$y) %>% round(2)
```

Data 4

```{r}
sd(data4$x) %>% round(2)
sd(data4$y) %>% round(2)
```

#### For each x and y pair, calculate (also to two decimal places; 1 pt):

#### d. The correlation (1 pt).

Data 1

```{r}
cor(data1$x, data1$y) %>% round(2)
```

Data 2

```{r}
cor(data2$x, data2$y) %>% round(2)
```

Data 3

```{r}
cor(data3$x, data3$y) %>% round(2)
```

Data 4
```{r}
cor(data4$x, data4$y) %>% round(2)
```
```

#### e. Linear regression equation (2 pts).

Data 1

```{r}
d1lm <- lm(y~x, data1)
summary(d1lm)
```

$$\widehat{y}=0.5001\times x + 3.0001$$

Data 2

```{r}
d2lm <- lm(y~x, data2)
summary(d2lm)
```

$$\widehat{y}=0.5001\times x + 3.0001$$

Data 3

```{r}
d3lm <- lm(y~x, data3)
summary(d3lm)
```

$$\widehat{y}=0.4997\times x + 3.0025$$

Data 4

```{r}
d4lm <- lm(y~x, data4)
summary(d4lm)
```

$$\widehat{y}=0.4999\times x + 3.0017$$

#### f. R-Squared (2 pts).

Dta 1

```{r}
print('Adjusted R-squared:  0.6295')
```

Data 2

```{r}
print('Adjusted R-squared:  0.6292')
```

Data 3
```{r}
print('Adjusted R-squared:  0.6292')
```

Data 4

```{r}
print('Adjusted R-squared:  0.6297')
```


#### For each pair, is it appropriate to estimate a linear regression model? Why or why not? Be specific as to why for each pair and include appropriate plots! (4 pts)

Data 1

3 conditions for inference must be met:

1: Linearity: Met

```{r, message=FALSE}
ggplot(data1, aes(x, y)) +
  geom_point() +
  geom_smooth(method=lm)
```

2: Nearly Normal Residuals: Met

```{r, message=FALSE}
qqnorm(d1lm$residuals)
qqline(d1lm$residuals)
```

3: Constant Variability: Met

```{r, message=FALSE}
ggplot(data1, aes(x, d1lm$residuals)) +
  geom_point()
```

As all the conditions for inference have been met, a linear regression model __is__ appropriate for Data 1

Data 2

3 conditions for inference must be met:

1: Linearity: Not Met

```{r, message=FALSE}
ggplot(data2, aes(x, y)) +
  geom_point() +
  geom_smooth(method=lm)
```

A linear regression model is __NOT__ appropriate for Data 2

Data 3

3 conditions for inference must be met:

1: Linearity: Not Met

```{r, message=FALSE}
ggplot(data3, aes(x, y)) +
  geom_point() +
  geom_smooth(method=lm)
```

2: Nearly Normal Residuals: Not Met

```{r, message=FALSE}
qqnorm(d3lm$residuals)
qqline(d3lm$residuals)
```

A linear regression model is __NOT__ appropriate for Data 3

Data 4

3 conditions for inference must be met:

1: Linearity: Not Met

```{r, message=FALSE}
ggplot(data4, aes(x, y)) +
  geom_point() +
  geom_smooth(method=lm)
```

2: Nearly Normal Residuals: Not Met

```{r, message=FALSE}
qqnorm(d3lm$residuals)
qqline(d3lm$residuals)
```


3: Constant Variability: Not Met

```{r, message=FALSE}
ggplot(data4, aes(x, d1lm$residuals)) +
  geom_point()
```

A linear regression model is __NOT__ appropriate for Data 4

#### Explain why it is important to include appropriate visualizations when analyzing data. Include any visualization(s) you create. (2 pts)

Visiualization are a critical tool for data analysis. For example, without visualizations, it would have been difficult to determine the very important differences in
the four data sets. All of the data sets critical values were nearly identical and they linear regression models were as well. It was not until the data (and their
respective conditions for inference) were plotted that the differences between the data sets became apparent.

All the calculations conducted above failed to accurately describe the differences between the data sets as well as the below graph:

```{r}
data1 %<>%
  mutate(type = 'data1')
data2 %<>%
  mutate(type = 'data2')
data3 %<>%
  mutate(type = 'data3')
data4 %<>%
  mutate(type = 'data4')
all.data <- rbind(data1, data2, data3, data4)
ggplot(all.data, aes(x, y)) +
  geom_point() +
  facet_wrap('type')
  
```


















